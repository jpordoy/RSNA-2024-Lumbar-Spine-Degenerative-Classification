{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d37f019",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-22T22:17:01.332140Z",
     "iopub.status.busy": "2024-09-22T22:17:01.331700Z",
     "iopub.status.idle": "2024-09-22T22:17:16.924782Z",
     "shell.execute_reply": "2024-09-22T22:17:16.923757Z"
    },
    "papermill": {
     "duration": 15.603777,
     "end_time": "2024-09-22T22:17:16.926881",
     "exception": false,
     "start_time": "2024-09-22T22:17:01.323104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Complete\n"
     ]
    }
   ],
   "source": [
    "# Basic Libraries\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow and Keras for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Attention, Add, Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "\n",
    "# Scikit-learn for Model Preparation\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Additional Libraries for Image Handling and File Operations\n",
    "import glob\n",
    "from glob import glob\n",
    "import matplotlib.image as mpimg\n",
    "import pydicom\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "\n",
    "\n",
    "print(\"Imports Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cb2686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:17:16.941650Z",
     "iopub.status.busy": "2024-09-22T22:17:16.941123Z",
     "iopub.status.idle": "2024-09-22T22:17:17.089717Z",
     "shell.execute_reply": "2024-09-22T22:17:17.088965Z"
    },
    "papermill": {
     "duration": 0.158204,
     "end_time": "2024-09-22T22:17:17.091993",
     "exception": false,
     "start_time": "2024-09-22T22:17:16.933789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\n",
    "df_train_series_descriptions = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv\")\n",
    "df_label_coord = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv\")\n",
    "#df_train_imagess = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n",
    "\n",
    "# Load datasets\n",
    "df_test_series_descriptions = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv\")\n",
    "#df_train_imagess = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n",
    "\n",
    "# Output Paths\n",
    "output_path = '/kaggle/working/train_images'\n",
    "# Define the directory where your augmented images are saved\n",
    "augmented_images_dir = '/kaggle/working/augmented_images'\n",
    "\n",
    "# Path to the input and output directories\n",
    "input_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "test_input_path= '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a4fbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:17:17.106124Z",
     "iopub.status.busy": "2024-09-22T22:17:17.105809Z",
     "iopub.status.idle": "2024-09-22T22:17:17.558105Z",
     "shell.execute_reply": "2024-09-22T22:17:17.557183Z"
    },
    "papermill": {
     "duration": 0.461692,
     "end_time": "2024-09-22T22:17:17.560254",
     "exception": false,
     "start_time": "2024-09-22T22:17:17.098562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>instance_number</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46411</th>\n",
       "      <td>4096820034</td>\n",
       "      <td>3236751045</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>5</td>\n",
       "      <td>Left Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>213.127193</td>\n",
       "      <td>70.175439</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         study_id   series_id series_description  instance_number  \\\n",
       "46411  4096820034  3236751045        Sagittal T1                5   \n",
       "\n",
       "                             condition  level           x          y  \\\n",
       "46411  Left Neural Foraminal Narrowing  L1/L2  213.127193  70.175439   \n",
       "\n",
       "                                              image_path     severity  \n",
       "46411  /kaggle/input/rsna-2024-lumbar-spine-degenerat...  Normal/Mild  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create image paths\n",
    "df_label_coord['image_path'] = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/\" + \\\n",
    "                               df_label_coord['study_id'].astype(str) + \"/\" + \\\n",
    "                               df_label_coord['series_id'].astype(str) + \"/\" + \\\n",
    "                               df_label_coord['instance_number'].astype(str) + \".dcm\"\n",
    "\n",
    "# Melt the df_train DataFrame\n",
    "df_train_melted = df_train.melt(id_vars=['study_id'], var_name='condition_level', value_name='severity')\n",
    "\n",
    "# Split 'condition_level' to extract 'condition' and 'level'\n",
    "df_train_melted[['conditions', 'level']] = df_train_melted['condition_level'].str.rsplit('_', n=2, expand=True).iloc[:, 1:]\n",
    "df_train_melted['condition'] = df_train_melted['condition_level'].apply(lambda x: '_'.join(x.split('_')[:-2])).str.replace(\"_\", \" \").str.title()\n",
    "df_train_melted['level'] = df_train_melted['conditions'].str.upper() + \"/\" + df_train_melted['level'].str.upper()\n",
    "\n",
    "# Drop the original 'condition_level' column\n",
    "df_train_melted = df_train_melted.drop(columns=['condition_level', 'conditions'])\n",
    "\n",
    "# Merge DataFrames on 'study_id', 'level', and 'condition'\n",
    "df_final = pd.merge(df_label_coord, df_train_melted, on=['study_id', 'level', 'condition'], how='inner')\n",
    "\n",
    "# Ensure the 'series_description' column exists before trying to reorder\n",
    "if 'series_description' in df_train_series_descriptions.columns:\n",
    "    # Merge df_final with df_train_series_descriptions on 'study_id' and 'series_id'\n",
    "    df_final_filtered = pd.merge(df_final, df_train_series_descriptions[['study_id', 'series_id', 'series_description']],\n",
    "                                 on=['study_id', 'series_id'], how='left')\n",
    "\n",
    "    # Reorder columns to place 'series_description' immediately after 'series_id'\n",
    "    columns_order = ['study_id', 'series_id', 'series_description', 'instance_number', 'condition', 'level', 'x', 'y', 'image_path', 'severity']\n",
    "    \n",
    "    # Ensure that 'series_description' exists in the DataFrame before reordering columns\n",
    "    if 'series_description' in df_final_filtered.columns:\n",
    "        df_final_filtered = df_final_filtered[columns_order]\n",
    "    else:\n",
    "        print(\"Warning: 'series_description' column not found after merging.\")\n",
    "else:\n",
    "    print(\"Warning: 'series_description' column not found in the input data.\")\n",
    "    \n",
    "df_final_filtered.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebdb95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:17:17.575854Z",
     "iopub.status.busy": "2024-09-22T22:17:17.575532Z",
     "iopub.status.idle": "2024-09-22T22:27:54.759829Z",
     "shell.execute_reply": "2024-09-22T22:27:54.758777Z"
    },
    "papermill": {
     "duration": 637.194214,
     "end_time": "2024-09-22T22:27:54.761922",
     "exception": false,
     "start_time": "2024-09-22T22:17:17.567708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24546 [00:00<?, ?it/s]/tmp/ipykernel_25/3440978783.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_png_paths = pd.concat([df_png_paths, pd.DataFrame([new_row])], ignore_index=True)\n",
      "100%|██████████| 24546/24546 [10:36<00:00, 38.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to PNG completed.\n",
      "Dataframe saved.\n"
     ]
    }
   ],
   "source": [
    "# Path to the output directory\n",
    "output_path = '/kaggle/working/train_images/'\n",
    "\n",
    "# Function to convert DICOM pixel array to PNG\n",
    "def readdcm_writepng_image(src_dicom_pixelarray, dest_path_png):\n",
    "    src_dicom_pixelarray = np.array(src_dicom_pixelarray)\n",
    "    standardized_image_data = ((src_dicom_pixelarray - src_dicom_pixelarray.min()) / \n",
    "                               (src_dicom_pixelarray.max() - src_dicom_pixelarray.min() + 1e-10)) * 255\n",
    "    standardized_image_data = standardized_image_data.astype(np.uint8)\n",
    "    final_image_to_png = cv2.resize(standardized_image_data, (320, 320), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(dest_path_png, final_image_to_png)\n",
    "\n",
    "# Remove previous output directory for fresh writing\n",
    "if os.path.isdir(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "# Drop duplicates based on 'image_path' to ensure each image is converted only once\n",
    "unique_images_df = df_final_filtered.drop_duplicates(subset='image_path')\n",
    "\n",
    "# Create a new DataFrame to store paths to the converted images\n",
    "df_png_paths = pd.DataFrame(columns=df_final_filtered.columns)\n",
    "\n",
    "# Convert only unique labeled images\n",
    "for index, row in tqdm(unique_images_df.iterrows(), total=len(unique_images_df)):\n",
    "    study_id = row['study_id']\n",
    "    # Apply the replacement to series_description\n",
    "    series_description = row['series_description'].replace(' ', '_').replace('/', '_')\n",
    "    instance_number = row['instance_number']\n",
    "    \n",
    "    # Construct the destination path for the PNG file\n",
    "    dest_path = f'{output_path}/{study_id}/{series_description}/{instance_number}.png'\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    \n",
    "    # Read the DICOM image and convert it to PNG\n",
    "    dicom_image = pydicom.dcmread(row['image_path'])\n",
    "    readdcm_writepng_image(dicom_image.pixel_array, dest_path)\n",
    "    \n",
    "    # Copy the row and update the image path to the new PNG path\n",
    "    new_row = row.copy()\n",
    "    new_row['image_path'] = dest_path\n",
    "    \n",
    "    # Replace series_description in the new_row DataFrame\n",
    "    new_row['series_description'] = series_description\n",
    "    \n",
    "    # Append the new row to the new DataFrame using pd.concat\n",
    "    df_png_paths = pd.concat([df_png_paths, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "print(\"Conversion to PNG completed.\")\n",
    "\n",
    "# Save the new DataFrame to a CSV file (optional)\n",
    "df_png_paths.to_csv('/kaggle/working/df_png_paths.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Dataframe saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ca3d20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:27:55.506899Z",
     "iopub.status.busy": "2024-09-22T22:27:55.506069Z",
     "iopub.status.idle": "2024-09-22T22:29:51.007904Z",
     "shell.execute_reply": "2024-09-22T22:29:51.006823Z"
    },
    "papermill": {
     "duration": 115.876626,
     "end_time": "2024-09-22T22:29:51.010065",
     "exception": false,
     "start_time": "2024-09-22T22:27:55.133439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "100%|██████████| 5418/5418 [01:53<00:00, 47.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed images: 31494\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, Rotate, RandomBrightnessContrast,\n",
    "    ColorJitter, GridDistortion, RandomGamma, GaussNoise, Compose,\n",
    "    CLAHE, Solarize, Posterize, ShiftScaleRotate, ElasticTransform,\n",
    "    ToGray, HueSaturationValue\n",
    ")\n",
    "\n",
    "# Step 1: Initialise Paths\n",
    "df_converted_data = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n",
    "output_images_dir = '/kaggle/working/augmented_images'\n",
    "csv_output_path = '/kaggle/working/df_augmented_final.csv'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_images_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Assume df_png_paths is already defined with the necessary data\n",
    "# You need to format the series_description\n",
    "df_augmented = df_png_paths.copy()\n",
    "df_augmented['series_description'] = df_augmented['series_description'].str.replace(r'[ /]', '_', regex=True)\n",
    "\n",
    "# Step 3: Define color map augmentation functions\n",
    "def apply_color_map(image, colormap):\n",
    "    return cv2.applyColorMap(image, colormap)\n",
    "\n",
    "# Step 4: Define augmentation techniques\n",
    "albumentations_augmentations = [\n",
    "    Compose([Rotate(limit=90), HorizontalFlip()]),\n",
    "    Compose([Rotate(limit=180)]),\n",
    "    Compose([Rotate(limit=270), HorizontalFlip()]),\n",
    "    Compose([ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)]),\n",
    "    Compose([GaussNoise(), VerticalFlip()]),\n",
    "    Compose([GridDistortion()]),\n",
    "    Compose([ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15)]),\n",
    "    Compose([ElasticTransform(alpha=1, sigma=50, alpha_affine=None)]),  # Updated line\n",
    "    Compose([CLAHE(), HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20)]),\n",
    "    Compose([Solarize(threshold=128.0), Posterize(num_bits=4)]),\n",
    "    Compose([ToGray()])\n",
    "]\n",
    "\n",
    "color_map_augmentations = [\n",
    "    (cv2.COLORMAP_VIRIDIS, 'viridis'),\n",
    "    (cv2.COLORMAP_PLASMA, 'plasma'),\n",
    "    (cv2.COLORMAP_INFERNO, 'inferno'),\n",
    "    (cv2.COLORMAP_MAGMA, 'magma'),\n",
    "]\n",
    "\n",
    "# Combine all augmentations into one list\n",
    "all_augmentations = albumentations_augmentations + color_map_augmentations\n",
    "\n",
    "# Define how many times to augment each image for Moderate and Severe classes\n",
    "num_augmentations_per_image_severe = 10 # Augment each 'Severe' image 6 times\n",
    "num_augmentations_per_image_moderate = 4  # Augment each 'Moderate' image 1 time\n",
    "\n",
    "def augment_image(row):\n",
    "    image_path = row['image_path']\n",
    "    image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Warning: Unable to load image at path: {image_path}\")\n",
    "        return []  # Return an empty list if the image could not be loaded\n",
    "\n",
    "    coords = [row['x'], row['y']]  # Extract coordinates\n",
    "    augmented_images = []  # Store augmented images for this row\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Determine the number of augmentations based on severity\n",
    "    if row['severity'] == 'Severe':\n",
    "        num_augmentations = num_augmentations_per_image_severe\n",
    "    elif row['severity'] == 'Moderate':\n",
    "        num_augmentations = num_augmentations_per_image_moderate\n",
    "    else:\n",
    "        return []  # Skip if severity is not 'Moderate' or 'Severe'\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Choose an augmentation\n",
    "        aug_index = np.random.choice(len(all_augmentations))\n",
    "        aug = all_augmentations[aug_index]\n",
    "\n",
    "        try:\n",
    "            if isinstance(aug, tuple):\n",
    "                # Apply the color map augmentation\n",
    "                colormap, name = aug\n",
    "                image_aug = apply_color_map(image, colormap)\n",
    "                aug_name = name  # Use the color map name directly\n",
    "            else:\n",
    "                # Apply the Albumentations augmentation\n",
    "                augmented = aug(image=image)\n",
    "                image_aug = augmented['image']\n",
    "\n",
    "                # Get the augmentation names\n",
    "                aug_name = '_'.join([type(t).__name__ for t in aug.transforms])\n",
    "\n",
    "                # Update coordinates based on the applied transformations\n",
    "                for t in aug.transforms:\n",
    "                    if isinstance(t, HorizontalFlip):\n",
    "                        coords[0] = image_width - coords[0]\n",
    "                    if isinstance(t, VerticalFlip):\n",
    "                        coords[1] = image_height - coords[1]\n",
    "                    if isinstance(t, Rotate):\n",
    "                        angle = t.limit if isinstance(t.limit, (int, float)) else t.limit[1]\n",
    "                        if angle == 90:\n",
    "                            coords = [coords[1], image_width - coords[0]]\n",
    "                        elif angle == 180:\n",
    "                            coords = [image_width - coords[0], image_height - coords[1]]\n",
    "                        elif angle == 270:\n",
    "                            coords = [image_height - coords[1], coords[0]]\n",
    "\n",
    "            # Create subfolder structure\n",
    "            study_id = row['study_id']\n",
    "            series_id = row['series_id']\n",
    "            series_description = row['series_description'].replace(' ', '_')  # Replace spaces with underscores\n",
    "            output_subfolder = os.path.join(output_images_dir, str(study_id), series_description)\n",
    "            os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "            # Generate new file name with the augmentation name and instance number\n",
    "            instance_number = row['instance_number']\n",
    "            augmented_image_path = os.path.join(output_subfolder, f\"{aug_name}_{instance_number}.png\")\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(augmented_image_path, image_aug)\n",
    "\n",
    "            augmented_images.append({\n",
    "                'study_id': study_id,\n",
    "                'series_id': series_id,\n",
    "                'series_description': series_description,\n",
    "                'instance_number': instance_number,\n",
    "                'x': coords[0],\n",
    "                'y': coords[1],\n",
    "                'condition': row['condition'],\n",
    "                'level': row['level'],\n",
    "                'image_path': augmented_image_path,\n",
    "                'severity': row['severity']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "# Step 7: Filter only Moderate and Severe classes for augmentation\n",
    "df_filtered = df_augmented[df_augmented['severity'].isin(['Moderate', 'Severe'])]\n",
    "\n",
    "# Step 8: Use parallel processing to augment images\n",
    "augmented_data = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(augment_image, [row for _, row in df_filtered.iterrows()]), total=len(df_filtered)))\n",
    "\n",
    "# Flatten the results and filter out None values\n",
    "augmented_data = [item for sublist in results for item in sublist if item is not None]\n",
    "\n",
    "# Step 9: Collect the results into a DataFrame\n",
    "df_augmented_final = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Save the augmented DataFrame to a CSV file\n",
    "df_augmented_final.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"Total processed images: {len(augmented_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3a2e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:51.850426Z",
     "iopub.status.busy": "2024-09-22T22:29:51.850028Z",
     "iopub.status.idle": "2024-09-22T22:29:51.931070Z",
     "shell.execute_reply": "2024-09-22T22:29:51.930139Z"
    },
    "papermill": {
     "duration": 0.502967,
     "end_time": "2024-09-22T22:29:51.933184",
     "exception": false,
     "start_time": "2024-09-22T22:29:51.430217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>instance_number</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal_T2_STIR</td>\n",
       "      <td>8</td>\n",
       "      <td>Spinal Canal Stenosis</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>322.831858</td>\n",
       "      <td>227.964602</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>4</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L4/L5</td>\n",
       "      <td>187.961759</td>\n",
       "      <td>251.839388</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>5</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L3/L4</td>\n",
       "      <td>187.227533</td>\n",
       "      <td>210.722753</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>6</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>194.569790</td>\n",
       "      <td>127.755258</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>11</td>\n",
       "      <td>Left Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>196.070671</td>\n",
       "      <td>126.021201</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24541</th>\n",
       "      <td>4290709089</td>\n",
       "      <td>3390218084</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>21</td>\n",
       "      <td>Right Subarticular Stenosis</td>\n",
       "      <td>L5/S1</td>\n",
       "      <td>302.875911</td>\n",
       "      <td>364.627811</td>\n",
       "      <td>/kaggle/working/train_images//4290709089/Axial...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24542</th>\n",
       "      <td>4290709089</td>\n",
       "      <td>4237840455</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>4</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L2/L3</td>\n",
       "      <td>208.106799</td>\n",
       "      <td>140.203404</td>\n",
       "      <td>/kaggle/working/train_images//4290709089/Sagit...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543</th>\n",
       "      <td>4290709089</td>\n",
       "      <td>4237840455</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>5</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>219.405706</td>\n",
       "      <td>95.459321</td>\n",
       "      <td>/kaggle/working/train_images//4290709089/Sagit...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>4290709089</td>\n",
       "      <td>4237840455</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>11</td>\n",
       "      <td>Left Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>219.465940</td>\n",
       "      <td>97.831063</td>\n",
       "      <td>/kaggle/working/train_images//4290709089/Sagit...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24545</th>\n",
       "      <td>4290709089</td>\n",
       "      <td>4237840455</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>12</td>\n",
       "      <td>Left Neural Foraminal Narrowing</td>\n",
       "      <td>L2/L3</td>\n",
       "      <td>205.340599</td>\n",
       "      <td>140.207084</td>\n",
       "      <td>/kaggle/working/train_images//4290709089/Sagit...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24546 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         study_id   series_id series_description  instance_number  \\\n",
       "0         4003253   702807833   Sagittal_T2_STIR                8   \n",
       "1         4003253  1054713880        Sagittal_T1                4   \n",
       "2         4003253  1054713880        Sagittal_T1                5   \n",
       "3         4003253  1054713880        Sagittal_T1                6   \n",
       "4         4003253  1054713880        Sagittal_T1               11   \n",
       "...           ...         ...                ...              ...   \n",
       "24541  4290709089  3390218084           Axial_T2               21   \n",
       "24542  4290709089  4237840455        Sagittal_T1                4   \n",
       "24543  4290709089  4237840455        Sagittal_T1                5   \n",
       "24544  4290709089  4237840455        Sagittal_T1               11   \n",
       "24545  4290709089  4237840455        Sagittal_T1               12   \n",
       "\n",
       "                              condition  level           x           y  \\\n",
       "0                 Spinal Canal Stenosis  L1/L2  322.831858  227.964602   \n",
       "1      Right Neural Foraminal Narrowing  L4/L5  187.961759  251.839388   \n",
       "2      Right Neural Foraminal Narrowing  L3/L4  187.227533  210.722753   \n",
       "3      Right Neural Foraminal Narrowing  L1/L2  194.569790  127.755258   \n",
       "4       Left Neural Foraminal Narrowing  L1/L2  196.070671  126.021201   \n",
       "...                                 ...    ...         ...         ...   \n",
       "24541       Right Subarticular Stenosis  L5/S1  302.875911  364.627811   \n",
       "24542  Right Neural Foraminal Narrowing  L2/L3  208.106799  140.203404   \n",
       "24543  Right Neural Foraminal Narrowing  L1/L2  219.405706   95.459321   \n",
       "24544   Left Neural Foraminal Narrowing  L1/L2  219.465940   97.831063   \n",
       "24545   Left Neural Foraminal Narrowing  L2/L3  205.340599  140.207084   \n",
       "\n",
       "                                              image_path     severity  \n",
       "0      /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild  \n",
       "1      /kaggle/working/train_images//4003253/Sagittal...     Moderate  \n",
       "2      /kaggle/working/train_images//4003253/Sagittal...     Moderate  \n",
       "3      /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild  \n",
       "4      /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild  \n",
       "...                                                  ...          ...  \n",
       "24541  /kaggle/working/train_images//4290709089/Axial...  Normal/Mild  \n",
       "24542  /kaggle/working/train_images//4290709089/Sagit...  Normal/Mild  \n",
       "24543  /kaggle/working/train_images//4290709089/Sagit...  Normal/Mild  \n",
       "24544  /kaggle/working/train_images//4290709089/Sagit...  Normal/Mild  \n",
       "24545  /kaggle/working/train_images//4290709089/Sagit...  Normal/Mild  \n",
       "\n",
       "[24546 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n",
    "final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd7acc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:52.812914Z",
     "iopub.status.busy": "2024-09-22T22:29:52.812265Z",
     "iopub.status.idle": "2024-09-22T22:29:52.831933Z",
     "shell.execute_reply": "2024-09-22T22:29:52.831001Z"
    },
    "papermill": {
     "duration": 0.440933,
     "end_time": "2024-09-22T22:29:52.833843",
     "exception": false,
     "start_time": "2024-09-22T22:29:52.392910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   study_id   series_id series_description\n",
      "0  44036939  2828203845        Sagittal_T1\n",
      "1  44036939  3481971518           Axial_T2\n",
      "2  44036939  3844393089   Sagittal_T2_STIR\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n",
    "#train  = pd.read_csv(train_path + 'train.csv')\n",
    "#label = pd.read_csv(train_path + 'train_label_coordinates.csv')\n",
    "train_desc  = pd.read_csv(train_path + 'train_series_descriptions.csv')\n",
    "test_desc   = pd.read_csv(train_path + 'test_series_descriptions.csv')\n",
    "#sub         = pd.read_csv(train_path + 'sample_submission.csv')\n",
    "\n",
    "# Replace spaces and forward slashes in the 'series_description' column with underscores\n",
    "test_desc['series_description'] = test_desc['series_description'].str.replace(r'[ /]', '_', regex=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(test_desc.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8198b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:53.674817Z",
     "iopub.status.busy": "2024-09-22T22:29:53.673938Z",
     "iopub.status.idle": "2024-09-22T22:29:56.616799Z",
     "shell.execute_reply": "2024-09-22T22:29:56.615585Z"
    },
    "papermill": {
     "duration": 3.366789,
     "end_time": "2024-09-22T22:29:56.618861",
     "exception": false,
     "start_time": "2024-09-22T22:29:53.252072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "output_path = '/kaggle/working/test_images/'\n",
    "\n",
    "# Function to convert DICOM pixel array to PNG\n",
    "def readdcm_writepng_image(src_dicom_pixelarray, dest_path_png):\n",
    "    src_dicom_pixelarray = np.array(src_dicom_pixelarray)\n",
    "    standardized_image_data = ((src_dicom_pixelarray - src_dicom_pixelarray.min()) / \n",
    "                               (src_dicom_pixelarray.max() - src_dicom_pixelarray.min() + 1e-10)) * 255\n",
    "    standardized_image_data = standardized_image_data.astype(np.uint8)\n",
    "    final_image_to_png = cv2.resize(standardized_image_data, (320, 320), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(dest_path_png, final_image_to_png)\n",
    "\n",
    "# Remove previous output directory for fresh writing\n",
    "if os.path.isdir(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "\n",
    "# Iterate over the test data\n",
    "for idx, row in tqdm(test_desc.iterrows(), total=len(test_desc)):\n",
    "    study_id = row['study_id']\n",
    "    series_id = row['series_id']\n",
    "    series_desc = row['series_description'].replace(' ', '_').replace('/', '_')\n",
    "    \n",
    "    # Define the new directory structure for PNGs\n",
    "    series_output_dir = f'{output_path}/{study_id}/{series_desc}'\n",
    "    os.makedirs(series_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all DICOM files in this series\n",
    "    series_dicom_dir = f'{test_input_path}/{study_id}/{series_id}'\n",
    "    dicom_files = glob(f'{series_dicom_dir}/*.dcm')\n",
    "    \n",
    "    # Convert each DICOM file to PNG\n",
    "    for dicom_file in dicom_files:\n",
    "        dicom_image = pydicom.dcmread(dicom_file)\n",
    "        image_filename = os.path.splitext(os.path.basename(dicom_file))[0]  # Use SOPInstanceUID for naming\n",
    "        image_dicom_pixelarray = dicom_image.pixel_array\n",
    "        \n",
    "        dest_path = f'{series_output_dir}/{image_filename}.png'\n",
    "        readdcm_writepng_image(image_dicom_pixelarray, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be5bb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:57.528871Z",
     "iopub.status.busy": "2024-09-22T22:29:57.528045Z",
     "iopub.status.idle": "2024-09-22T22:29:57.599185Z",
     "shell.execute_reply": "2024-09-22T22:29:57.598221Z"
    },
    "papermill": {
     "duration": 0.511751,
     "end_time": "2024-09-22T22:29:57.601320",
     "exception": false,
     "start_time": "2024-09-22T22:29:57.089569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>instance_number</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "      <th>severity</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>702807833</td>\n",
       "      <td>Sagittal_T2_STIR</td>\n",
       "      <td>8</td>\n",
       "      <td>Spinal Canal Stenosis</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>322.831858</td>\n",
       "      <td>227.964602</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>4003253_spinal_canal_stenosis_l1_l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>4</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L4/L5</td>\n",
       "      <td>187.961759</td>\n",
       "      <td>251.839388</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>5</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L3/L4</td>\n",
       "      <td>187.227533</td>\n",
       "      <td>210.722753</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>6</td>\n",
       "      <td>Right Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>194.569790</td>\n",
       "      <td>127.755258</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>4003253_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1054713880</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>11</td>\n",
       "      <td>Left Neural Foraminal Narrowing</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>196.070671</td>\n",
       "      <td>126.021201</td>\n",
       "      <td>/kaggle/working/train_images//4003253/Sagittal...</td>\n",
       "      <td>Normal/Mild</td>\n",
       "      <td>4003253_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description  instance_number  \\\n",
       "0   4003253   702807833   Sagittal_T2_STIR                8   \n",
       "1   4003253  1054713880        Sagittal_T1                4   \n",
       "2   4003253  1054713880        Sagittal_T1                5   \n",
       "3   4003253  1054713880        Sagittal_T1                6   \n",
       "4   4003253  1054713880        Sagittal_T1               11   \n",
       "\n",
       "                          condition  level           x           y  \\\n",
       "0             Spinal Canal Stenosis  L1/L2  322.831858  227.964602   \n",
       "1  Right Neural Foraminal Narrowing  L4/L5  187.961759  251.839388   \n",
       "2  Right Neural Foraminal Narrowing  L3/L4  187.227533  210.722753   \n",
       "3  Right Neural Foraminal Narrowing  L1/L2  194.569790  127.755258   \n",
       "4   Left Neural Foraminal Narrowing  L1/L2  196.070671  126.021201   \n",
       "\n",
       "                                          image_path     severity  \\\n",
       "0  /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild   \n",
       "1  /kaggle/working/train_images//4003253/Sagittal...     Moderate   \n",
       "2  /kaggle/working/train_images//4003253/Sagittal...     Moderate   \n",
       "3  /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild   \n",
       "4  /kaggle/working/train_images//4003253/Sagittal...  Normal/Mild   \n",
       "\n",
       "                                           row_id  \n",
       "0             4003253_spinal_canal_stenosis_l1_l2  \n",
       "1  4003253_right_neural_foraminal_narrowing_l4_l5  \n",
       "2  4003253_right_neural_foraminal_narrowing_l3_l4  \n",
       "3  4003253_right_neural_foraminal_narrowing_l1_l2  \n",
       "4   4003253_left_neural_foraminal_narrowing_l1_l2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the row_id column\n",
    "final_merged_df['row_id'] = (\n",
    "    final_merged_df['study_id'].astype(str) + '_' +\n",
    "    final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n",
    "    final_merged_df['level'].str.lower().str.replace('/', '_')\n",
    ")\n",
    "\n",
    "# Note: Check image path, since there's 1 instance id, for 1 image, but there's many more images other than the ones labelled in the instance ID. \n",
    "\n",
    "# Display the updated dataframe\n",
    "final_merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8c8b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:58.450619Z",
     "iopub.status.busy": "2024-09-22T22:29:58.449942Z",
     "iopub.status.idle": "2024-09-22T22:29:58.484950Z",
     "shell.execute_reply": "2024-09-22T22:29:58.484091Z"
    },
    "papermill": {
     "duration": 0.45684,
     "end_time": "2024-09-22T22:29:58.487005",
     "exception": false,
     "start_time": "2024-09-22T22:29:58.030165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>image_path</th>\n",
       "      <th>condition</th>\n",
       "      <th>row_id</th>\n",
       "      <th>instance_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Sagittal_...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Sagittal_...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Sagittal_...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Sagittal_...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal_T1</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Sagittal_...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description  \\\n",
       "0  44036939  2828203845        Sagittal_T1   \n",
       "1  44036939  2828203845        Sagittal_T1   \n",
       "2  44036939  2828203845        Sagittal_T1   \n",
       "3  44036939  2828203845        Sagittal_T1   \n",
       "4  44036939  2828203845        Sagittal_T1   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /kaggle/working/test_images/44036939/Sagittal_...   \n",
       "1  /kaggle/working/test_images/44036939/Sagittal_...   \n",
       "2  /kaggle/working/test_images/44036939/Sagittal_...   \n",
       "3  /kaggle/working/test_images/44036939/Sagittal_...   \n",
       "4  /kaggle/working/test_images/44036939/Sagittal_...   \n",
       "\n",
       "                         condition  \\\n",
       "0  left_neural_foraminal_narrowing   \n",
       "1  left_neural_foraminal_narrowing   \n",
       "2  left_neural_foraminal_narrowing   \n",
       "3  left_neural_foraminal_narrowing   \n",
       "4  left_neural_foraminal_narrowing   \n",
       "\n",
       "                                           row_id  instance_number  \n",
       "0  44036939_left_neural_foraminal_narrowing_l1_l2               17  \n",
       "1  44036939_left_neural_foraminal_narrowing_l2_l3               15  \n",
       "2  44036939_left_neural_foraminal_narrowing_l3_l4                1  \n",
       "3  44036939_left_neural_foraminal_narrowing_l4_l5               10  \n",
       "4  44036939_left_neural_foraminal_narrowing_l5_s1               20  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base path for test images\n",
    "base_path = '/kaggle/working/test_images'\n",
    "\n",
    "# Function to get image paths for a series\n",
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_description']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n",
    "    return []\n",
    "\n",
    "# Mapping of series_description to conditions\n",
    "condition_mapping = {\n",
    "    'Sagittal_T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial_T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal_T2_STIR': 'spinal_canal_stenosis'\n",
    "}\n",
    "\n",
    "# Create a list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Expand the dataframe by adding new rows for each file path\n",
    "for index, row in test_desc.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n",
    "\n",
    "# Create a new dataframe from the expanded rows\n",
    "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Extracting the instance number from the image_path\n",
    "expanded_test_desc['instance_number'] = expanded_test_desc['image_path'].apply(\n",
    "    lambda x: int(os.path.splitext(os.path.basename(x))[0])  # Get the filename without extension\n",
    ")\n",
    "\n",
    "levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n",
    "\n",
    "# Function to update row_id with levels\n",
    "def update_row_id(row, levels):\n",
    "    level = levels[row.name % len(levels)]\n",
    "    return f\"{row['study_id']}_{row['condition']}_{level}\"\n",
    "\n",
    "# Update row_id in expanded_test_desc to include levels\n",
    "expanded_test_desc['row_id'] = expanded_test_desc.apply(lambda row: update_row_id(row, levels), axis=1)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "expanded_test_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ce3b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:29:59.367315Z",
     "iopub.status.busy": "2024-09-22T22:29:59.366918Z",
     "iopub.status.idle": "2024-09-22T22:29:59.410615Z",
     "shell.execute_reply": "2024-09-22T22:29:59.409489Z"
    },
    "papermill": {
     "duration": 0.468305,
     "end_time": "2024-09-22T22:29:59.412725",
     "exception": false,
     "start_time": "2024-09-22T22:29:58.944420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing rows with severity 0 or NaN: 24526 samples\n",
      "Data after removing rows with severity 0 or NaN: 31494 samples\n",
      "severity\n",
      "Normal/Mild    19108\n",
      "Moderate       18905\n",
      "Severe         18007\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with severity equal to 0 or NaN\n",
    "df_final_filtered_cleaned = df_png_paths[(df_png_paths['severity'] != 0) & (df_png_paths['severity'].notna())]\n",
    "df_augmented_cleaned = df_augmented_final[(df_augmented_final['severity'] != 0) & (df_augmented_final['severity'].notna())]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(f\"Data after removing rows with severity 0 or NaN: {df_final_filtered_cleaned.shape[0]} samples\")\n",
    "print(f\"Data after removing rows with severity 0 or NaN: {df_augmented_cleaned.shape[0]} samples\")\n",
    "\n",
    "# Concatenate the cleaned DataFrames\n",
    "df_concat = pd.concat([df_final_filtered_cleaned, df_augmented_cleaned], ignore_index=True)\n",
    "\n",
    "# Check the class distribution after balancing\n",
    "print(df_concat[\"severity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d7c627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:30:00.275051Z",
     "iopub.status.busy": "2024-09-22T22:30:00.274298Z",
     "iopub.status.idle": "2024-09-22T22:30:22.972863Z",
     "shell.execute_reply": "2024-09-22T22:30:22.971612Z"
    },
    "papermill": {
     "duration": 23.12245,
     "end_time": "2024-09-22T22:30:22.974898",
     "exception": false,
     "start_time": "2024-09-22T22:29:59.852448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupted files removed: 0\n",
      "Number of valid rows in the final DataFrame: 56020\n"
     ]
    }
   ],
   "source": [
    "# List to store paths of corrupted files\n",
    "corrupted_files = []\n",
    "\n",
    "# Check each image in the dataset\n",
    "for index, row in df_concat.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    try:\n",
    "        # Try to open the image file\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()  # Verify that it is a valid image\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        corrupted_files.append(img_path)\n",
    "\n",
    "# Remove corrupted files from the DataFrame\n",
    "df_concat_cleaned = df_concat[~df_concat['image_path'].isin(corrupted_files)]\n",
    "\n",
    "# Create the final augmented DataFrame with cleaned data\n",
    "df_dataset = df_concat_cleaned.copy()\n",
    "\n",
    "# Print the number of corrupted files found and removed\n",
    "print(f\"Number of corrupted files removed: {len(corrupted_files)}\")\n",
    "\n",
    "# Print the number of valid rows in the final DataFrame\n",
    "print(f\"Number of valid rows in the final DataFrame: {df_dataset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458c2625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:30:23.865866Z",
     "iopub.status.busy": "2024-09-22T22:30:23.865486Z",
     "iopub.status.idle": "2024-09-22T22:30:23.880059Z",
     "shell.execute_reply": "2024-09-22T22:30:23.879175Z"
    },
    "papermill": {
     "duration": 0.438601,
     "end_time": "2024-09-22T22:30:23.881853",
     "exception": false,
     "start_time": "2024-09-22T22:30:23.443252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition\n",
       "Left Subarticular Stenosis          26059\n",
       "Right Subarticular Stenosis         11184\n",
       "Right Neural Foraminal Narrowing     7560\n",
       "Left Neural Foraminal Narrowing      7233\n",
       "Spinal Canal Stenosis                3984\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[\"condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc1519",
   "metadata": {
    "papermill": {
     "duration": 0.44139,
     "end_time": "2024-09-22T22:30:24.744900",
     "exception": false,
     "start_time": "2024-09-22T22:30:24.303510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ca208",
   "metadata": {
    "papermill": {
     "duration": 0.459893,
     "end_time": "2024-09-22T22:30:25.629609",
     "exception": false,
     "start_time": "2024-09-22T22:30:25.169716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5ce5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:30:26.490439Z",
     "iopub.status.busy": "2024-09-22T22:30:26.489945Z",
     "iopub.status.idle": "2024-09-22T22:30:26.505771Z",
     "shell.execute_reply": "2024-09-22T22:30:26.504791Z"
    },
    "papermill": {
     "duration": 0.46242,
     "end_time": "2024-09-22T22:30:26.508159",
     "exception": false,
     "start_time": "2024-09-22T22:30:26.045739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = df_dataset['series_description'].nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095a0049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:30:27.440129Z",
     "iopub.status.busy": "2024-09-22T22:30:27.439728Z",
     "iopub.status.idle": "2024-09-22T22:54:09.623397Z",
     "shell.execute_reply": "2024-09-22T22:54:09.622374Z"
    },
    "papermill": {
     "duration": 1422.651591,
     "end_time": "2024-09-22T22:54:09.625426",
     "exception": false,
     "start_time": "2024-09-22T22:30:26.973835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for series description: Sagittal_T2_STIR\n",
      "Found 2230 validated image filenames belonging to 3 classes.\n",
      "Found 558 validated image filenames belonging to 3 classes.\n",
      "Found 1195 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727044231.086880      82 service.cc:145] XLA service 0x7b74f00021a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727044231.086938      82 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1727044231.086943      82 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3/70\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.4115 - loss: 1.4284"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727044238.325854      82 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 204ms/step - accuracy: 0.5763 - loss: 0.9780 - val_accuracy: 0.6631 - val_loss: 0.7532\n",
      "Epoch 2/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.6963 - loss: 0.7158 - val_accuracy: 0.7384 - val_loss: 0.6246\n",
      "Epoch 3/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.8239 - loss: 0.4529 - val_accuracy: 0.7957 - val_loss: 0.5553\n",
      "Epoch 4/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 116ms/step - accuracy: 0.9209 - loss: 0.2290 - val_accuracy: 0.8423 - val_loss: 0.4989\n",
      "Epoch 5/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.9578 - loss: 0.1303 - val_accuracy: 0.8405 - val_loss: 0.5413\n",
      "Epoch 6/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9786 - loss: 0.0679 - val_accuracy: 0.8799 - val_loss: 0.5907\n",
      "Epoch 7/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 117ms/step - accuracy: 0.9837 - loss: 0.0442 - val_accuracy: 0.8799 - val_loss: 0.6037\n",
      "Epoch 8/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.9925 - loss: 0.0366 - val_accuracy: 0.8943 - val_loss: 0.6038\n",
      "Epoch 9/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - accuracy: 0.9971 - loss: 0.0104 - val_accuracy: 0.8871 - val_loss: 0.7248\n",
      "Epoch 10/10\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.9994 - loss: 0.0036 - val_accuracy: 0.8871 - val_loss: 0.7084\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 0.8860 - loss: 0.7628\n",
      "Test Accuracy for Sagittal_T2_STIR: 0.8803\n",
      "\n",
      "Training model for series description: Sagittal_T1\n",
      "Found 8284 validated image filenames belonging to 3 classes.\n",
      "Found 2071 validated image filenames belonging to 3 classes.\n",
      "Found 4439 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 152ms/step - accuracy: 0.5371 - loss: 1.0223 - val_accuracy: 0.5949 - val_loss: 0.8432\n",
      "Epoch 2/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 125ms/step - accuracy: 0.6067 - loss: 0.8330 - val_accuracy: 0.6296 - val_loss: 0.7675\n",
      "Epoch 3/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 120ms/step - accuracy: 0.7345 - loss: 0.6137 - val_accuracy: 0.7475 - val_loss: 0.6035\n",
      "Epoch 4/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 119ms/step - accuracy: 0.8676 - loss: 0.3447 - val_accuracy: 0.8146 - val_loss: 0.5374\n",
      "Epoch 5/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 124ms/step - accuracy: 0.9415 - loss: 0.1711 - val_accuracy: 0.8310 - val_loss: 0.5619\n",
      "Epoch 6/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 121ms/step - accuracy: 0.9699 - loss: 0.0911 - val_accuracy: 0.8450 - val_loss: 0.6752\n",
      "Epoch 7/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.9863 - loss: 0.0448 - val_accuracy: 0.8522 - val_loss: 0.8359\n",
      "Epoch 8/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 120ms/step - accuracy: 0.9936 - loss: 0.0302 - val_accuracy: 0.8662 - val_loss: 0.9627\n",
      "Epoch 9/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 123ms/step - accuracy: 0.9968 - loss: 0.0158 - val_accuracy: 0.8436 - val_loss: 1.0064\n",
      "Epoch 10/10\n",
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 122ms/step - accuracy: 0.9968 - loss: 0.0200 - val_accuracy: 0.8556 - val_loss: 1.0878\n",
      "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 99ms/step - accuracy: 0.8413 - loss: 1.2454\n",
      "Test Accuracy for Sagittal_T1: 0.8459\n",
      "\n",
      "Training model for series description: Axial_T2\n",
      "Found 20856 validated image filenames belonging to 3 classes.\n",
      "Found 5214 validated image filenames belonging to 3 classes.\n",
      "Found 11173 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 157ms/step - accuracy: 0.5107 - loss: 0.9827 - val_accuracy: 0.6638 - val_loss: 0.7005\n",
      "Epoch 2/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 144ms/step - accuracy: 0.7175 - loss: 0.6206 - val_accuracy: 0.7558 - val_loss: 0.5543\n",
      "Epoch 3/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 136ms/step - accuracy: 0.8570 - loss: 0.3658 - val_accuracy: 0.8163 - val_loss: 0.4713\n",
      "Epoch 4/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 134ms/step - accuracy: 0.9356 - loss: 0.1840 - val_accuracy: 0.8360 - val_loss: 0.4816\n",
      "Epoch 5/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 134ms/step - accuracy: 0.9691 - loss: 0.0963 - val_accuracy: 0.8613 - val_loss: 0.4707\n",
      "Epoch 6/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 133ms/step - accuracy: 0.9846 - loss: 0.0607 - val_accuracy: 0.8640 - val_loss: 0.6046\n",
      "Epoch 7/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 134ms/step - accuracy: 0.9868 - loss: 0.0531 - val_accuracy: 0.8707 - val_loss: 0.5987\n",
      "Epoch 8/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 140ms/step - accuracy: 0.9897 - loss: 0.0445 - val_accuracy: 0.8700 - val_loss: 0.6304\n",
      "Epoch 9/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 139ms/step - accuracy: 0.9923 - loss: 0.0327 - val_accuracy: 0.8717 - val_loss: 0.6610\n",
      "Epoch 10/10\n",
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 136ms/step - accuracy: 0.9913 - loss: 0.0397 - val_accuracy: 0.8608 - val_loss: 0.7463\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.7451\n",
      "Test Accuracy for Axial_T2: 0.8734\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First convolutional block\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second convolutional block\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Third convolutional block\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output layer for 3 classes (severity)\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set parameters\n",
    "image_size = (256, 256)  # Target size for images\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_shape = (*image_size, 3)  # Assuming RGB images\n",
    "num_classes = 3  # Severity has 3 categories\n",
    "\n",
    "# Assuming df_dataset is your DataFrame containing the dataset with 'severity' and 'series_description'\n",
    "\n",
    "# Get unique series_descriptions from the DataFrame\n",
    "series_descriptions = df_dataset['series_description'].unique()\n",
    "\n",
    "# Iterate through each series_description\n",
    "for series in series_descriptions:\n",
    "    print(f\"\\nTraining model for series description: {series}\")\n",
    "    \n",
    "    # Filter the DataFrame for the current series_description\n",
    "    series_df = df_dataset[df_dataset['series_description'] == series]\n",
    "    \n",
    "    # Ensure we have 3 classes in the 'severity' column\n",
    "    assert series_df['severity'].nunique() == 3, \"The 'severity' column must have exactly 3 classes\"\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_df, test_df = train_test_split(series_df, test_size=0.3, stratify=series_df['severity'])\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['severity'])\n",
    "\n",
    "    # ImageDataGenerator setup\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "    # Create ImageDataGenerators for train, validation, and test sets\n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='image_path',\n",
    "        y_col='severity',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='image_path',\n",
    "        y_col='severity',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='image_path',\n",
    "        y_col='severity',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Create CNN model for the current series_description\n",
    "    model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the model after training\n",
    "    model.save(f\"{series}_model.h5\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_generator)\n",
    "    print(f\"Test Accuracy for {series}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f39cecc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:54:12.320947Z",
     "iopub.status.busy": "2024-09-22T22:54:12.319640Z",
     "iopub.status.idle": "2024-09-22T22:54:12.340063Z",
     "shell.execute_reply": "2024-09-22T22:54:12.339007Z"
    },
    "papermill": {
     "duration": 1.382606,
     "end_time": "2024-09-22T22:54:12.342044",
     "exception": false,
     "start_time": "2024-09-22T22:54:10.959438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>image_path</th>\n",
       "      <th>condition</th>\n",
       "      <th>row_id</th>\n",
       "      <th>instance_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>left_subarticular_stenosis</td>\n",
       "      <td>44036939_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>left_subarticular_stenosis</td>\n",
       "      <td>44036939_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>left_subarticular_stenosis</td>\n",
       "      <td>44036939_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>left_subarticular_stenosis</td>\n",
       "      <td>44036939_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>left_subarticular_stenosis</td>\n",
       "      <td>44036939_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>right_subarticular_stenosis</td>\n",
       "      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>right_subarticular_stenosis</td>\n",
       "      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>right_subarticular_stenosis</td>\n",
       "      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>right_subarticular_stenosis</td>\n",
       "      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>44036939</td>\n",
       "      <td>3481971518</td>\n",
       "      <td>Axial_T2</td>\n",
       "      <td>/kaggle/working/test_images/44036939/Axial_T2/...</td>\n",
       "      <td>right_subarticular_stenosis</td>\n",
       "      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     study_id   series_id series_description  \\\n",
       "50   44036939  3481971518           Axial_T2   \n",
       "51   44036939  3481971518           Axial_T2   \n",
       "52   44036939  3481971518           Axial_T2   \n",
       "53   44036939  3481971518           Axial_T2   \n",
       "54   44036939  3481971518           Axial_T2   \n",
       "..        ...         ...                ...   \n",
       "139  44036939  3481971518           Axial_T2   \n",
       "140  44036939  3481971518           Axial_T2   \n",
       "141  44036939  3481971518           Axial_T2   \n",
       "142  44036939  3481971518           Axial_T2   \n",
       "143  44036939  3481971518           Axial_T2   \n",
       "\n",
       "                                            image_path  \\\n",
       "50   /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "51   /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "52   /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "53   /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "54   /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "..                                                 ...   \n",
       "139  /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "140  /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "141  /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "142  /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "143  /kaggle/working/test_images/44036939/Axial_T2/...   \n",
       "\n",
       "                       condition                                      row_id  \\\n",
       "50    left_subarticular_stenosis   44036939_left_subarticular_stenosis_l1_l2   \n",
       "51    left_subarticular_stenosis   44036939_left_subarticular_stenosis_l2_l3   \n",
       "52    left_subarticular_stenosis   44036939_left_subarticular_stenosis_l3_l4   \n",
       "53    left_subarticular_stenosis   44036939_left_subarticular_stenosis_l4_l5   \n",
       "54    left_subarticular_stenosis   44036939_left_subarticular_stenosis_l5_s1   \n",
       "..                           ...                                         ...   \n",
       "139  right_subarticular_stenosis  44036939_right_subarticular_stenosis_l5_s1   \n",
       "140  right_subarticular_stenosis  44036939_right_subarticular_stenosis_l1_l2   \n",
       "141  right_subarticular_stenosis  44036939_right_subarticular_stenosis_l2_l3   \n",
       "142  right_subarticular_stenosis  44036939_right_subarticular_stenosis_l3_l4   \n",
       "143  right_subarticular_stenosis  44036939_right_subarticular_stenosis_l4_l5   \n",
       "\n",
       "     instance_number  \n",
       "50                17  \n",
       "51                15  \n",
       "52                 1  \n",
       "53                35  \n",
       "54                10  \n",
       "..               ...  \n",
       "139               29  \n",
       "140               19  \n",
       "141               21  \n",
       "142               13  \n",
       "143                3  \n",
       "\n",
       "[94 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Assuming 'expanded_test_desc' already exists and contains the image paths and row IDs\n",
    "# Create a directory for the models\n",
    "models_directory = '/kaggle/working'  # Update with your actual models directory\n",
    "\n",
    "# Initialize an empty list to collect predictions\n",
    "submission_data = []\n",
    "\n",
    "# Get unique series descriptions from the existing DataFrame\n",
    "series_descriptions = expanded_test_desc['series_description'].unique()\n",
    "series_df = expanded_test_desc[expanded_test_desc['series_description'] == series]\n",
    "series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf85918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:54:14.943265Z",
     "iopub.status.busy": "2024-09-22T22:54:14.942547Z",
     "iopub.status.idle": "2024-09-22T22:54:18.999399Z",
     "shell.execute_reply": "2024-09-22T22:54:18.998404Z"
    },
    "papermill": {
     "duration": 5.393338,
     "end_time": "2024-09-22T22:54:19.001385",
     "exception": false,
     "start_time": "2024-09-22T22:54:13.608047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing series: Sagittal_T1\n",
      "Found 194 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "\n",
      "Processing series: Axial_T2\n",
      "Found 194 validated image filenames.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "\n",
      "Processing series: Sagittal_T2_STIR\n",
      "Found 194 validated image filenames.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step\n",
      "Submission DataFrame created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Loop through each unique series description\n",
    "for series in series_descriptions:\n",
    "    print(f\"\\nProcessing series: {series}\")\n",
    "    \n",
    "    # Load the corresponding model\n",
    "    model_path = os.path.join(models_directory, f\"{series}_model.h5\")\n",
    "    \n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model for {series}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Filter the DataFrame for the current series\n",
    "    series_df = expanded_test_desc[expanded_test_desc['series_description'] == series]\n",
    "    \n",
    "    # Create a test data generator\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=expanded_test_desc,\n",
    "        x_col='image_path',\n",
    "        y_col=None,  # No labels for test data\n",
    "        class_mode=None,\n",
    "        target_size=(256, 256),  # Adjust to your model's expected input size\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "\n",
    "    # Convert predictions to probabilities\n",
    "    for idx, row in enumerate(series_df.itertuples()):\n",
    "        row_id = row.row_id\n",
    "        pred = predictions[idx]\n",
    "\n",
    "        # Assuming the model outputs class scores for normal/mild, moderate, and severe\n",
    "        normal_mild_prob = pred[0]  # Replace with appropriate index if necessary\n",
    "        moderate_prob = pred[1]      # Replace with appropriate index if necessary\n",
    "        severe_prob = pred[2]        # Replace with appropriate index if necessary\n",
    "\n",
    "        # Append the results to the submission data\n",
    "        submission_data.append({\n",
    "            'row_id': row_id,\n",
    "            'normal_mild': normal_mild_prob,\n",
    "            'moderate': moderate_prob,\n",
    "            'severe': severe_prob\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "\n",
    "# Set the display format for floating-point numbers to show decimals\n",
    "pd.options.display.float_format = '{:.8f}'.format  # Change the number of decimal places as needed\n",
    "\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission DataFrame created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5a2084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:54:21.612037Z",
     "iopub.status.busy": "2024-09-22T22:54:21.611123Z",
     "iopub.status.idle": "2024-09-22T22:54:21.619891Z",
     "shell.execute_reply": "2024-09-22T22:54:21.619028Z"
    },
    "papermill": {
     "duration": 1.316335,
     "end_time": "2024-09-22T22:54:21.621875",
     "exception": false,
     "start_time": "2024-09-22T22:54:20.305540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id\n",
       "44036939_spinal_canal_stenosis_l4_l5               10\n",
       "44036939_spinal_canal_stenosis_l5_s1               10\n",
       "44036939_spinal_canal_stenosis_l3_l4               10\n",
       "44036939_spinal_canal_stenosis_l2_l3               10\n",
       "44036939_spinal_canal_stenosis_l1_l2               10\n",
       "44036939_right_subarticular_stenosis_l4_l5         10\n",
       "44036939_right_subarticular_stenosis_l3_l4         10\n",
       "44036939_left_subarticular_stenosis_l2_l3          10\n",
       "44036939_left_subarticular_stenosis_l1_l2          10\n",
       "44036939_right_subarticular_stenosis_l5_s1          9\n",
       "44036939_left_subarticular_stenosis_l4_l5           9\n",
       "44036939_left_subarticular_stenosis_l3_l4           9\n",
       "44036939_left_subarticular_stenosis_l5_s1           9\n",
       "44036939_right_subarticular_stenosis_l1_l2          9\n",
       "44036939_right_subarticular_stenosis_l2_l3          9\n",
       "44036939_right_neural_foraminal_narrowing_l4_l5     5\n",
       "44036939_left_neural_foraminal_narrowing_l2_l3      5\n",
       "44036939_left_neural_foraminal_narrowing_l1_l2      5\n",
       "44036939_right_neural_foraminal_narrowing_l3_l4     5\n",
       "44036939_right_neural_foraminal_narrowing_l2_l3     5\n",
       "44036939_right_neural_foraminal_narrowing_l1_l2     5\n",
       "44036939_left_neural_foraminal_narrowing_l5_s1      5\n",
       "44036939_left_neural_foraminal_narrowing_l4_l5      5\n",
       "44036939_left_neural_foraminal_narrowing_l3_l4      5\n",
       "44036939_right_neural_foraminal_narrowing_l5_s1     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_test_desc[\"row_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79d003d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:54:24.163793Z",
     "iopub.status.busy": "2024-09-22T22:54:24.163409Z",
     "iopub.status.idle": "2024-09-22T22:54:24.187758Z",
     "shell.execute_reply": "2024-09-22T22:54:24.186901Z"
    },
    "papermill": {
     "duration": 1.31954,
     "end_time": "2024-09-22T22:54:24.189701",
     "exception": false,
     "start_time": "2024-09-22T22:54:22.870161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.97956097</td>\n",
       "      <td>0.02040968</td>\n",
       "      <td>0.00002937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.61017174</td>\n",
       "      <td>0.37367210</td>\n",
       "      <td>0.01615617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.80060089</td>\n",
       "      <td>0.00013241</td>\n",
       "      <td>0.19926673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.80551499</td>\n",
       "      <td>0.19313395</td>\n",
       "      <td>0.00135103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.62907904</td>\n",
       "      <td>0.17090245</td>\n",
       "      <td>0.20001845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.83883113</td>\n",
       "      <td>0.14804865</td>\n",
       "      <td>0.01312015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.70172036</td>\n",
       "      <td>0.00236970</td>\n",
       "      <td>0.29590994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.60849208</td>\n",
       "      <td>0.36634341</td>\n",
       "      <td>0.02516453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.75732207</td>\n",
       "      <td>0.20700628</td>\n",
       "      <td>0.03567173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.78804559</td>\n",
       "      <td>0.15518790</td>\n",
       "      <td>0.05676653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.97956097</td>\n",
       "      <td>0.02040968</td>\n",
       "      <td>0.00002937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.61017174</td>\n",
       "      <td>0.37367210</td>\n",
       "      <td>0.01615617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.80060089</td>\n",
       "      <td>0.00013241</td>\n",
       "      <td>0.19926673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.80551499</td>\n",
       "      <td>0.19313395</td>\n",
       "      <td>0.00135103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.62907904</td>\n",
       "      <td>0.17090245</td>\n",
       "      <td>0.20001845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.29213086</td>\n",
       "      <td>0.45855534</td>\n",
       "      <td>0.24931383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.20676357</td>\n",
       "      <td>0.31802416</td>\n",
       "      <td>0.47521231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.19468015</td>\n",
       "      <td>0.52050668</td>\n",
       "      <td>0.28481317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.33926296</td>\n",
       "      <td>0.49552885</td>\n",
       "      <td>0.16520821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.20325014</td>\n",
       "      <td>0.39748675</td>\n",
       "      <td>0.39926311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.20092019</td>\n",
       "      <td>0.71232307</td>\n",
       "      <td>0.08675675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.75232995</td>\n",
       "      <td>0.21023457</td>\n",
       "      <td>0.03743547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.02724868</td>\n",
       "      <td>0.80356610</td>\n",
       "      <td>0.16918522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.67138350</td>\n",
       "      <td>0.18149105</td>\n",
       "      <td>0.14712547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.01948898</td>\n",
       "      <td>0.91457415</td>\n",
       "      <td>0.06593693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             row_id  normal_mild   moderate  \\\n",
       "0    44036939_left_neural_foraminal_narrowing_l1_l2   0.97956097 0.02040968   \n",
       "1    44036939_left_neural_foraminal_narrowing_l2_l3   0.61017174 0.37367210   \n",
       "2    44036939_left_neural_foraminal_narrowing_l3_l4   0.80060089 0.00013241   \n",
       "3    44036939_left_neural_foraminal_narrowing_l4_l5   0.80551499 0.19313395   \n",
       "4    44036939_left_neural_foraminal_narrowing_l5_s1   0.62907904 0.17090245   \n",
       "5         44036939_left_subarticular_stenosis_l1_l2   0.83883113 0.14804865   \n",
       "6         44036939_left_subarticular_stenosis_l2_l3   0.70172036 0.00236970   \n",
       "7         44036939_left_subarticular_stenosis_l3_l4   0.60849208 0.36634341   \n",
       "8         44036939_left_subarticular_stenosis_l4_l5   0.75732207 0.20700628   \n",
       "9         44036939_left_subarticular_stenosis_l5_s1   0.78804559 0.15518790   \n",
       "10  44036939_right_neural_foraminal_narrowing_l1_l2   0.97956097 0.02040968   \n",
       "11  44036939_right_neural_foraminal_narrowing_l2_l3   0.61017174 0.37367210   \n",
       "12  44036939_right_neural_foraminal_narrowing_l3_l4   0.80060089 0.00013241   \n",
       "13  44036939_right_neural_foraminal_narrowing_l4_l5   0.80551499 0.19313395   \n",
       "14  44036939_right_neural_foraminal_narrowing_l5_s1   0.62907904 0.17090245   \n",
       "15       44036939_right_subarticular_stenosis_l1_l2   0.29213086 0.45855534   \n",
       "16       44036939_right_subarticular_stenosis_l2_l3   0.20676357 0.31802416   \n",
       "17       44036939_right_subarticular_stenosis_l3_l4   0.19468015 0.52050668   \n",
       "18       44036939_right_subarticular_stenosis_l4_l5   0.33926296 0.49552885   \n",
       "19       44036939_right_subarticular_stenosis_l5_s1   0.20325014 0.39748675   \n",
       "20             44036939_spinal_canal_stenosis_l1_l2   0.20092019 0.71232307   \n",
       "21             44036939_spinal_canal_stenosis_l2_l3   0.75232995 0.21023457   \n",
       "22             44036939_spinal_canal_stenosis_l3_l4   0.02724868 0.80356610   \n",
       "23             44036939_spinal_canal_stenosis_l4_l5   0.67138350 0.18149105   \n",
       "24             44036939_spinal_canal_stenosis_l5_s1   0.01948898 0.91457415   \n",
       "\n",
       "       severe  \n",
       "0  0.00002937  \n",
       "1  0.01615617  \n",
       "2  0.19926673  \n",
       "3  0.00135103  \n",
       "4  0.20001845  \n",
       "5  0.01312015  \n",
       "6  0.29590994  \n",
       "7  0.02516453  \n",
       "8  0.03567173  \n",
       "9  0.05676653  \n",
       "10 0.00002937  \n",
       "11 0.01615617  \n",
       "12 0.19926673  \n",
       "13 0.00135103  \n",
       "14 0.20001845  \n",
       "15 0.24931383  \n",
       "16 0.47521231  \n",
       "17 0.28481317  \n",
       "18 0.16520821  \n",
       "19 0.39926311  \n",
       "20 0.08675675  \n",
       "21 0.03743547  \n",
       "22 0.16918522  \n",
       "23 0.14712547  \n",
       "24 0.06593693  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count occurrences of each unique row_id\n",
    "row_id_counts = submission_df['row_id'].value_counts()\n",
    "\n",
    "# Group by 'row_id' and calculate the mean for each group\n",
    "mean_df = submission_df.groupby('row_id').mean().reset_index()\n",
    "\n",
    "# Display the mean DataFrame\n",
    "mean_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2250.543381,
   "end_time": "2024-09-22T22:54:28.959473",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-22T22:16:58.416092",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
