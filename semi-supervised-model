{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic Libraries\nimport os\nimport shutil\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Image Processing\nimport cv2\nfrom PIL import Image, ImageEnhance\nfrom skimage.util import random_noise\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# TensorFlow and Keras for Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Attention, Add, Dense, GlobalAveragePooling2D, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Scikit-learn for Model Preparation\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\n\n# Additional Libraries for Image Handling and File Operations\nimport glob\nimport matplotlib.image as mpimg\nimport pydicom\n\nprint(\"Imports Complete\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T13:27:52.871576Z","iopub.execute_input":"2024-09-04T13:27:52.871957Z","iopub.status.idle":"2024-09-04T13:28:06.336994Z","shell.execute_reply.started":"2024-09-04T13:27:52.871917Z","shell.execute_reply":"2024-09-04T13:28:06.336052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Imports Complete\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load datasets\ndf_train = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\ndf_train_series_descriptions = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv\")\ndf_label_coord = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv\")\n#df_train_imagess = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n\n# Load datasets\ndf_test_series_descriptions = pd.read_csv(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv\")\n#df_train_imagess = pd.read_csv(\"/kaggle/working/df_png_paths.csv\")\n\n# Output Paths\noutput_path = '/kaggle/working/train_images'\n# Define the directory where your augmented images are saved\naugmented_images_dir = '/kaggle/working/augmented_images'\n\n# Path to the input and output directories\ninput_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\ntest_input_path= '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:28:09.054349Z","iopub.execute_input":"2024-09-04T13:28:09.055006Z","iopub.status.idle":"2024-09-04T13:28:09.202010Z","shell.execute_reply.started":"2024-09-04T13:28:09.054962Z","shell.execute_reply":"2024-09-04T13:28:09.201163Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create image paths\ndf_label_coord['image_path'] = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/\" + \\\n                               df_label_coord['study_id'].astype(str) + \"/\" + \\\n                               df_label_coord['series_id'].astype(str) + \"/\" + \\\n                               df_label_coord['instance_number'].astype(str) + \".dcm\"\n\n# Melt the df_train DataFrame\ndf_train_melted = df_train.melt(id_vars=['study_id'], var_name='condition_level', value_name='severity')\n\n# Split 'condition_level' to extract 'condition' and 'level'\ndf_train_melted[['conditions', 'level']] = df_train_melted['condition_level'].str.rsplit('_', n=2, expand=True).iloc[:, 1:]\ndf_train_melted['condition'] = df_train_melted['condition_level'].apply(lambda x: '_'.join(x.split('_')[:-2])).str.replace(\"_\", \" \").str.title()\ndf_train_melted['level'] = df_train_melted['conditions'].str.upper() + \"/\" + df_train_melted['level'].str.upper()\n\n# Drop the original 'condition_level' column\ndf_train_melted = df_train_melted.drop(columns=['condition_level', 'conditions'])\n\n# Merge DataFrames on 'study_id', 'level', and 'condition'\ndf_final = pd.merge(df_label_coord, df_train_melted, on=['study_id', 'level', 'condition'], how='inner')\n\n# Ensure the 'series_description' column exists before trying to reorder\nif 'series_description' in df_train_series_descriptions.columns:\n    # Merge df_final with df_train_series_descriptions on 'study_id' and 'series_id'\n    df_final_filtered = pd.merge(df_final, df_train_series_descriptions[['study_id', 'series_id', 'series_description']],\n                                 on=['study_id', 'series_id'], how='left')\n\n    # Reorder columns to place 'series_description' immediately after 'series_id'\n    columns_order = ['study_id', 'series_id', 'series_description', 'instance_number', 'condition', 'level', 'x', 'y', 'image_path', 'severity']\n    \n    # Ensure that 'series_description' exists in the DataFrame before reordering columns\n    if 'series_description' in df_final_filtered.columns:\n        df_final_filtered = df_final_filtered[columns_order]\n    else:\n        print(\"Warning: 'series_description' column not found after merging.\")\nelse:\n    print(\"Warning: 'series_description' column not found in the input data.\")\n    \ndf_final_filtered.sample()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:28:13.112908Z","iopub.execute_input":"2024-09-04T13:28:13.113919Z","iopub.status.idle":"2024-09-04T13:28:13.571677Z","shell.execute_reply.started":"2024-09-04T13:28:13.113862Z","shell.execute_reply":"2024-09-04T13:28:13.570743Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         study_id   series_id series_description  instance_number  \\\n42704  3760831283  1580868138           Axial T2                6   \n\n                         condition  level           x          y  \\\n42704  Right Subarticular Stenosis  L2/L3  300.099075  343.24967   \n\n                                              image_path     severity  \n42704  /kaggle/input/rsna-2024-lumbar-spine-degenerat...  Normal/Mild  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id</th>\n      <th>series_description</th>\n      <th>instance_number</th>\n      <th>condition</th>\n      <th>level</th>\n      <th>x</th>\n      <th>y</th>\n      <th>image_path</th>\n      <th>severity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42704</th>\n      <td>3760831283</td>\n      <td>1580868138</td>\n      <td>Axial T2</td>\n      <td>6</td>\n      <td>Right Subarticular Stenosis</td>\n      <td>L2/L3</td>\n      <td>300.099075</td>\n      <td>343.24967</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>Normal/Mild</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Path to store converted PNG images\ninput_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n\n# Function to convert DICOM pixel array to PNG\ndef readdcm_writepng_image(src_dicom_pixelarray, dest_path_png):\n    src_dicom_pixelarray = np.array(src_dicom_pixelarray)\n    standardized_image_data = ((src_dicom_pixelarray - src_dicom_pixelarray.min()) / \n                               (src_dicom_pixelarray.max() - src_dicom_pixelarray.min() + 1e-10)) * 255\n    standardized_image_data = standardized_image_data.astype(np.uint8)\n    final_image_to_png = cv2.resize(standardized_image_data, (320, 320), interpolation=cv2.INTER_CUBIC)\n    cv2.imwrite(dest_path_png, final_image_to_png)\n\n# Remove previous output directory for fresh writing\nif os.path.isdir(output_path):\n    shutil.rmtree(output_path)\n\n# Create a new DataFrame to store paths to the converted images\ndf_png_paths = pd.DataFrame(columns=['study_id', 'series_id', 'instance_number', 'image_path', \n                                     'condition', 'level', 'x', 'y', 'DataType', 'severity'])\n\n# Convert labeled images\nfor index, row in tqdm(df_final_filtered.iterrows(), total=len(df_final_filtered)):\n    study_id = row['study_id']\n    series_description = row['series_description'].replace(' ', '_').replace('/', '_')\n    instance_number = row['instance_number']\n    \n    # Construct the destination path for the PNG file\n    dest_path = os.path.join(output_path, str(study_id), series_description, f\"{instance_number}.png\")\n    \n    # Ensure directory exists\n    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n    \n    # Read the DICOM image and convert it to PNG\n    dicom_image = pydicom.dcmread(row['image_path'])\n    readdcm_writepng_image(dicom_image.pixel_array, dest_path)\n    \n    # Copy the row and update the image path to the new PNG path\n    new_row = row.copy()\n    new_row['image_path'] = dest_path\n    new_row['DataType'] = 'Labeled'  # Mark as labeled\n    \n    # Append the new row to the DataFrame\n    df_png_paths = pd.concat([df_png_paths, pd.DataFrame([new_row])], ignore_index=True)\n\n# Convert unlabeled images\n# Traverse the input directory to find all DICOM files\nfor root, _, files in os.walk(input_path):\n    for file in files:\n        if file.endswith('.dcm'):  # Check if the file is a DICOM file\n            dcm_path = os.path.join(root, file)\n            # Extract study_id and series_id from the file path\n            parts = dcm_path.split(os.sep)\n            if len(parts) > 7:  # Ensure there are enough parts in the path\n                study_id = parts[5]\n                series_id = parts[6].replace(' ', '_').replace('/', '_')\n                instance_number = file.split('.')[0]  # Use file name without extension as instance number\n\n                # Construct the destination path for the PNG file\n                dest_path = os.path.join(output_path, study_id, series_id, f\"{instance_number}.png\")\n\n                # Ensure directory exists\n                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n\n                # Read the DICOM image and convert it to PNG\n                dicom_image = pydicom.dcmread(dcm_path)\n                readdcm_writepng_image(dicom_image.pixel_array, dest_path)\n\n                # Create a new row for the unlabeled image\n                new_row_unlabeled = {\n                    'study_id': study_id,\n                    'series_id': series_id,\n                    'instance_number': instance_number,\n                    'image_path': dest_path,\n                    'condition': 0,  # Placeholder for unlabeled data\n                    'level': 0,     # Placeholder for unlabeled data\n                    'x': 0,         # Placeholder for x coordinate\n                    'y': 0,         # Placeholder for y coordinate\n                    'DataType': 'Unlabeled', # Mark as unlabeled\n                    'severity': 0  # Placeholder for severity\n\n                }\n\n                # Append the new row to the DataFrame\n                df_png_paths = pd.concat([df_png_paths, pd.DataFrame([new_row_unlabeled])], ignore_index=True)\n\nprint(\"Conversion to PNG and DataFrame update completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T13:29:26.997979Z","iopub.execute_input":"2024-09-04T13:29:26.998474Z","iopub.status.idle":"2024-09-04T15:21:20.191180Z","shell.execute_reply.started":"2024-09-04T13:29:26.998432Z","shell.execute_reply":"2024-09-04T15:21:20.190140Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"  0%|          | 0/48692 [00:00<?, ?it/s]/tmp/ipykernel_37/3475820108.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df_png_paths = pd.concat([df_png_paths, pd.DataFrame([new_row])], ignore_index=True)\n100%|██████████| 48692/48692 [20:11<00:00, 40.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Conversion to PNG and DataFrame update completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Separate the labeled and unlabeled data into two DataFrames\nlabeled_df = df_png_paths[df_png_paths['DataType'] == 'Labeled'].reset_index(drop=True)\nunlabeled_df = df_png_paths[df_png_paths['DataType'] == 'Unlabeled'].reset_index(drop=True)\n\n# Drop the 'DataType' column from both DataFrames\nlabeled_df = labeled_df.drop(columns=['DataType'])\nunlabeled_df = unlabeled_df.drop(columns=['DataType'])\n\n# Display the number of records in each DataFrame\nprint(f\"Number of labeled images: {labeled_df.shape[0]}\")\nprint(f\"Number of unlabeled images: {unlabeled_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:32:39.820762Z","iopub.execute_input":"2024-09-04T15:32:39.821684Z","iopub.status.idle":"2024-09-04T15:32:40.049767Z","shell.execute_reply.started":"2024-09-04T15:32:39.821643Z","shell.execute_reply":"2024-09-04T15:32:40.048850Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of labeled images: 48692\nNumber of unlabeled images: 147218\n","output_type":"stream"}]},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, Rotate, RandomBrightnessContrast,\n    ColorJitter, GridDistortion, RandomGamma, GaussNoise, Compose,\n    CLAHE, Solarize, Posterize, ShiftScaleRotate, ElasticTransform,\n    ToGray, HueSaturationValue\n)\n\n# Step 1: Initialise Paths\noutput_images_dir = '/kaggle/working/augmented_images'\ncsv_output_path = '/kaggle/working/df_augmented_final.csv'\n\n# Ensure output directory exists\nos.makedirs(output_images_dir, exist_ok=True)\n\n# Step 2: Assume df_png_paths is already defined with the necessary data\n# You need to format the series_description\ndf_augmented = labeled_df.copy()\ndf_augmented['series_description'] = df_augmented['series_description'].str.replace(r'[ /]', '_', regex=True)\n\n# Step 3: Define color map augmentation functions\ndef apply_color_map(image, colormap):\n    return cv2.applyColorMap(image, colormap)\n\n# Step 4: Define augmentation techniques\nalbumentations_augmentations = [\n    Compose([Rotate(limit=90), HorizontalFlip()]),\n    Compose([Rotate(limit=180)]),\n    Compose([Rotate(limit=270), HorizontalFlip()]),\n    Compose([ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)]),\n    Compose([GaussNoise(), VerticalFlip()]),\n    Compose([GridDistortion()]),\n    Compose([ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15)]),\n    Compose([ElasticTransform(alpha=1, sigma=50, alpha_affine=None)]),  # Updated line\n    Compose([CLAHE(), HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20)]),\n    Compose([Solarize(threshold=128.0), Posterize(num_bits=4)]),\n    Compose([ToGray()])\n]\n\n\ncolor_map_augmentations = [\n    (cv2.COLORMAP_VIRIDIS, 'viridis'),\n    (cv2.COLORMAP_PLASMA, 'plasma'),\n    (cv2.COLORMAP_INFERNO, 'inferno'),\n    (cv2.COLORMAP_MAGMA, 'magma'),\n]\n\n# Combine all augmentations into one list\nall_augmentations = albumentations_augmentations + color_map_augmentations\n\n# Define how many times to augment each image for Moderate and Severe classes\nnum_augmentations_per_image =  6 # Augment each image 10 times\n\n# Step 6: Augment a single image and return results\ndef augment_image(row):\n    image_path = row['image_path']\n    image = cv2.imread(image_path)  # Load the image using OpenCV\n    coords = [row['x'], row['y']]  # Extract coordinates\n    augmented_images = []  # Store augmented images for this row\n    image_height, image_width = image.shape[:2]\n\n    for _ in range(num_augmentations_per_image):\n        # Choose an augmentation\n        aug_index = np.random.choice(len(all_augmentations))\n        aug = all_augmentations[aug_index]\n\n        try:\n            if isinstance(aug, tuple):\n                # Apply the color map augmentation\n                colormap, name = aug\n                image_aug = apply_color_map(image, colormap)\n                aug_name = name  # Use the color map name directly\n            else:\n                # Apply the Albumentations augmentation\n                augmented = aug(image=image)\n                image_aug = augmented['image']\n\n                # Get the augmentation names\n                aug_name = '_'.join([type(t).__name__ for t in aug.transforms])\n\n                # Update coordinates based on the applied transformations\n                for t in aug.transforms:\n                    if isinstance(t, HorizontalFlip):\n                        coords[0] = image_width - coords[0]\n                    if isinstance(t, VerticalFlip):\n                        coords[1] = image_height - coords[1]\n                    if isinstance(t, Rotate):\n                        angle = t.limit if isinstance(t.limit, (int, float)) else t.limit[1]\n                        if angle == 90:\n                            coords = [coords[1], image_width - coords[0]]\n                        elif angle == 180:\n                            coords = [image_width - coords[0], image_height - coords[1]]\n                        elif angle == 270:\n                            coords = [image_height - coords[1], coords[0]]\n\n            # Create subfolder structure\n            study_id = row['study_id']\n            series_description = row['series_description'].replace(' ', '_')  # Replace spaces with underscores\n            output_subfolder = os.path.join(output_images_dir, str(study_id), series_description)\n            os.makedirs(output_subfolder, exist_ok=True)\n\n            # Generate new file name with the augmentation name and instance number\n            instance_number = row['instance_number']\n            augmented_image_path = os.path.join(output_subfolder, f\"{aug_name}_{instance_number}.png\")\n\n            # Save the augmented image\n            cv2.imwrite(augmented_image_path, image_aug)\n\n            augmented_images.append({\n                'study_id': study_id,\n                'series_description': series_description,\n                'instance_number': instance_number,\n                'x': coords[0],\n                'y': coords[1],\n                'condition': row['condition'],\n                'level': row['level'],\n                'image_path': augmented_image_path,\n                'severity': row['severity']\n            })\n        except Exception as e:\n            print(f\"Error processing image {image_path}: {e}\")\n\n    return augmented_images\n\n\n\n# Step 7: Filter only Moderate and Severe classes for augmentation\ndf_filtered = df_augmented[df_augmented['severity'].isin(['Moderate', 'Severe'])]\n\n# Step 8: Use parallel processing to augment images\naugmented_data = []\n\nwith ThreadPoolExecutor() as executor:\n    results = list(tqdm(executor.map(augment_image, [row for _, row in df_filtered.iterrows()]), total=len(df_filtered)))\n\n# Flatten the results and filter out None values\naugmented_data = [item for sublist in results for item in sublist if item is not None]\n\n# Step 9: Collect the results into a DataFrame\ndf_augmented_final = pd.DataFrame(augmented_data)\n\n# Save the augmented DataFrame to a CSV file\ndf_augmented_final.to_csv(csv_output_path, index=False)\n\nprint(f\"Total processed images: {len(augmented_data)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:33:31.207249Z","iopub.execute_input":"2024-09-04T15:33:31.207638Z","iopub.status.idle":"2024-09-04T15:37:48.901240Z","shell.execute_reply.started":"2024-09-04T15:33:31.207601Z","shell.execute_reply":"2024-09-04T15:37:48.900327Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 11031/11031 [04:14<00:00, 43.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total processed images: 66186\n","output_type":"stream"}]},{"cell_type":"code","source":"# Drop rows with severity equal to 0 or NaN\nlabeled_df_cleaned = labeled_df[(labeled_df['severity'] != 0) & (labeled_df['severity'].notna())]\ndf_augmented_cleaned = df_augmented_final[(df_augmented_final['severity'] != 0) & (df_augmented_final['severity'].notna())]\n\n# Display the resulting DataFrame\nprint(f\"Data after removing rows with severity 0 or NaN: {labeled_df_cleaned.shape[0]} samples\")\nprint(f\"Data after removing rows with severity 0 or NaN: {df_augmented_cleaned.shape[0]} samples\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:41:05.109775Z","iopub.execute_input":"2024-09-04T15:41:05.110843Z","iopub.status.idle":"2024-09-04T15:41:05.158925Z","shell.execute_reply.started":"2024-09-04T15:41:05.110790Z","shell.execute_reply":"2024-09-04T15:41:05.158036Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Data after removing rows with severity 0 or NaN: 48657 samples\nData after removing rows with severity 0 or NaN: 66186 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"df_concat = pd.concat([labeled_df_cleaned, df_augmented_cleaned], ignore_index=True)\ndf_concat[\"severity\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:41:44.165278Z","iopub.execute_input":"2024-09-04T15:41:44.166533Z","iopub.status.idle":"2024-09-04T15:41:44.213756Z","shell.execute_reply.started":"2024-09-04T15:41:44.166480Z","shell.execute_reply":"2024-09-04T15:41:44.212875Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"severity\nModerate       55650\nNormal/Mild    37626\nSevere         21567\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\n\n# Assuming df_concat is already loaded and contains the labeled_dfdata\n\n# 1. Drop rows with NaN in 'severity'\ndf_concat = df_concat.dropna(subset=['severity'])\n\n# 2. Determine the minimum class count for undersampling\nclass_counts = df_concat['severity'].value_counts()\nmin_count = class_counts.min()\n\nprint(\"Class distribution before undersampling:\")\nprint(class_counts)\n\n# 3. Prepare a list to hold the undersampled data\ndf_resampled_list = []\n\n# 4. Undersample each class to match the minimum count\nfor cls in class_counts.index:\n    # Get all rows of the current class\n    class_df = df_concat[df_concat['severity'] == cls]\n    \n    # Undersample the class to match the minimum count\n    class_undersampled = resample(class_df,\n                                  replace=False,  # Sample without replacement\n                                  n_samples=min_count,  # Match the minimum class size\n                                  random_state=42)  # For reproducibility\n    \n    # Append the undersampled class to the list\n    df_resampled_list.append(class_undersampled)\n\n# Concatenate all undersampled DataFrames\ndf_resampled = pd.concat(df_resampled_list)\n\n# Shuffle the DataFrame to mix the order of rows\ndf_resampled = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Class distribution after undersampling:\")\nprint(df_resampled['severity'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:42:56.847762Z","iopub.execute_input":"2024-09-04T15:42:56.848649Z","iopub.status.idle":"2024-09-04T15:42:57.108834Z","shell.execute_reply.started":"2024-09-04T15:42:56.848605Z","shell.execute_reply":"2024-09-04T15:42:57.107786Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Class distribution before undersampling:\nseverity\nModerate       55650\nNormal/Mild    37626\nSevere         21567\nName: count, dtype: int64\nClass distribution after undersampling:\nseverity\nNormal/Mild    21567\nModerate       21567\nSevere         21567\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\n\n# Assuming df_resampled is your DataFrame containing image paths\ncorrupt_images = []\n\n# Loop through the DataFrame and check each image\nfor idx, row in df_resampled.iterrows():\n    img_path = row['image_path']\n    try:\n        with Image.open(img_path) as img:\n            img.verify()  # Verify that it is an image\n    except (IOError, OSError, SyntaxError):\n        corrupt_images.append(img_path)\n\n# Count and remove corrupt images from the DataFrame\ncorrupt_count = len(corrupt_images)\ndf_resampled = df_resampled[~df_resampled['image_path'].isin(corrupt_images)]\n\n# Output the count of corrupt images and the updated DataFrame\nprint(f\"Number of corrupt images found and removed: {corrupt_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:45:36.710836Z","iopub.execute_input":"2024-09-04T15:45:36.711532Z","iopub.status.idle":"2024-09-04T15:46:21.911218Z","shell.execute_reply.started":"2024-09-04T15:45:36.711491Z","shell.execute_reply":"2024-09-04T15:46:21.910250Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of corrupt images found and removed: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Step 1: Split data into training (75%) and test sets (25%)\ntrain_df, test_df = train_test_split(df_resampled, test_size=0.25, random_state=42, stratify=df_resampled['severity'])\n\n# Step 2: Split the training set into a smaller training set and a validation set\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['severity'])\n\n# Explanation:","metadata":{"execution":{"iopub.status.busy":"2024-09-04T21:43:58.031799Z","iopub.execute_input":"2024-09-04T21:43:58.032808Z","iopub.status.idle":"2024-09-04T21:43:58.270274Z","shell.execute_reply.started":"2024-09-04T21:43:58.032754Z","shell.execute_reply":"2024-09-04T21:43:58.269452Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-04T15:48:34.944027Z","iopub.execute_input":"2024-09-04T15:48:34.944419Z","iopub.status.idle":"2024-09-04T15:48:34.949472Z","shell.execute_reply.started":"2024-09-04T15:48:34.944380Z","shell.execute_reply":"2024-09-04T15:48:34.948555Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Assuming `df` is your original DataFrame with image paths and labels\ntrain_df, test_df = train_test_split(df_resampled, test_size=0.3, stratify=df_resampled['severity'])\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['severity'])\n\n# Create ImageDataGenerators\ndatagen = ImageDataGenerator(rescale=1./255)\n\n# Create data generators in one line\ntrain_generator, val_generator, test_generator = (\n    datagen.flow_from_dataframe(\n        dataframe=df_split,\n        x_col='image_path',\n        y_col='severity',\n        target_size=(224, 224),\n        batch_size=128 if df_split is train_df else 128,\n        class_mode='categorical',\n        shuffle=(df_split is train_df)\n    )\n    for df_split in (train_df, val_df, test_df)\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T21:44:56.664168Z","iopub.execute_input":"2024-09-04T21:44:56.664610Z","iopub.status.idle":"2024-09-04T21:44:57.721783Z","shell.execute_reply.started":"2024-09-04T21:44:56.664570Z","shell.execute_reply":"2024-09-04T21:44:57.720795Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Found 36122 validated image filenames belonging to 3 classes.\nFound 9031 validated image filenames belonging to 3 classes.\nFound 19352 validated image filenames belonging to 3 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.losses import mean_squared_error,mean_absolute_error\nfrom keras.optimizers import Nadam\nfrom keras.optimizers import AdamW,RMSprop\n\n\ndef create_cnn_attention_model(input_shape=(224, 224, 3)):\n    inputs = layers.Input(shape=input_shape)\n\n    # Convolutional layers with Batch Normalization and Dropout\n    x = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.25)(x)\n\n    x = layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.25)(x)\n\n    x = layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Flatten before attention mechanism\n    x = layers.Flatten()(x)\n\n    # Attention mechanism\n    x = layers.Reshape((1, -1))(x)\n    attention_weights = layers.Attention()([x, x])\n    x = layers.add([x, attention_weights])\n\n    # Final dense layer with softmax activation\n    x = layers.Flatten()(x)\n    x = layers.Dense(128, activation='relu')(x)  # Add more dense layers if needed\n    x = layers.Dropout(0.25)(x)\n    outputs = layers.Dense(3, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n\n    return model\n\n# Example usage\nmodel = create_cnn_attention_model(input_shape=(224, 224, 3))\n\n# Set hyperparameters\nepochs = 10  # Adjust as needed\nverbose = 2  # Adjust verbosity level (0 for silent, 1 for progress bar, 2 for one line per epoch)\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.5,  # Reduce learning rate by a factor of 0.5\n    patience=3,  # Number of epochs with no improvement to wait before reducing the learning rate\n    min_lr=1e-6,  # Lower bound on learning rate\n    verbose=1\n)\n\n# Train the model with callbacks\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=val_generator,\n    shuffle=True,\n    callbacks=[reduce_lr],\n    verbose=verbose\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T21:45:01.173628Z","iopub.execute_input":"2024-09-04T21:45:01.173992Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"283/283 - 236s - 834ms/step - accuracy: 0.4253 - loss: 0.3820 - val_accuracy: 0.3325 - val_loss: 0.4449 - learning_rate: 1.0000e-04\nEpoch 2/10\n283/283 - 182s - 643ms/step - accuracy: 0.3346 - loss: 0.4434 - val_accuracy: 0.3299 - val_loss: 0.4466 - learning_rate: 1.0000e-04\nEpoch 3/10\n283/283 - 181s - 640ms/step - accuracy: 0.3360 - loss: 0.4427 - val_accuracy: 0.3324 - val_loss: 0.4451 - learning_rate: 1.0000e-04\nEpoch 4/10\n","output_type":"stream"}]}]}